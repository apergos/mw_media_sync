Test archiving now that we have the whole 'look up every project type
by api' code.

Test running without a todo list, even if we might never use that.

Test on more active wikis (with more uploads locally/remotely and more
file deletions) the generation of diffs for deletes, project uploads,
foreign repo uploads.

This general approach is very inefficient, downloading and processing the same files
again although we already have them, either for an earlier run today or a previous
run. Keeping track of the most recent files, doing HEAD requests would be nice.

Separate sessions for each download so we are not keeping a connection tied
up on these servers. Do we want to batch a few downloads at a time or is
being nice to the remote host priority?

Be prepared to try to download a bunch of stuff that doesn't exist (because links
in a gallery on a project can contain anything and those things may not actually
be existing files); find a nice way to handle these cases.

Once we have deleted all downloaded files not in use any more on the remote side,
we should be able to keep some sort of diff, 'here are new files on the remote
that have been deleted' by comparing the last two lists of remote files and
seeing what's been removed. Then we can look just for those newly deleted files
locally and get rid of them if we have them. MUCH cheaper.

Once we have downloaded all files in use on the remote side, we should be able
to generate a list of NEW files only, by comparing last two lists, and just
get those, without all this headache and walking directory trees and statting
a billion files etc. There will be the timestamp issue however; we need also
a way to generate a list of all files that have had new versions uploaded since
the last time, both uploaded to the project repo and uploaded to the foreign
repo.

We still have to deal with timestamps for foreign repo files, which we don't have,
so we can't compare versions.

We might try to download the same nonexistent links on every run. What should
we do about that? It's possible the files will be there the next time (someone
might have uploaded them).  Should we refuse to retry failures for some number
of runs (configurable)?

We should be able to specify 'deletes only' so we don't have to do all the
steps for a run.

Make --continue also look for most recent incremental 'get these' lists
in the case that the most recent successful download is from one of those

So, lots to do, even after a first kinda-working version is done.
