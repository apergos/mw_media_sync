This approach is very inefficient, downloading and processing the same files
again although we already have them, either for an earlier run today or a previous
run. Keeping track of the most recent files, doing HEAD requests would be nice.

Separate sessions for each download so we are not keeping a connection tied
up on these servers. Do we want to batch a few downloads at a time or is
being nice to the remote host priority?

Be prepared to try to download a bunch of stuff that doesn't exist (because links
in a gallery on a project can contain anything and those things may not actually
be existing files); find a nice way to handle these cases.

Once we have deleted all downloaded files not in use any more on the remote side,
we should be able to keep some sort of diff, 'here are new files on the remote
that have been deleted' by comparing the last two lists of remote files and
seeing what's been removed. Then we can look just for those newly deleted files
locally and get rid of them if we have them. MUCH cheaper.

Once we have downloaded all files in use on the remote side, we should be able
to generate a list of NEW files only, by comparing last two lists, and just
get those, without all this headache and walking directory trees and statting
a billion files etc. There will be the timestamp issue however; we need also
a way to generate a list of all files that have had new versions uploaded since
the last time, both uploaded to the project repo and uploaded to the foreign
repo.

We still have to deal with timestamps for foreign repo files, which we don't have,
so we can't compare versions.

We overwrite log files if we run on a wiki more than once in a day. Maybe we should
append. Gzip lets you just tack on a new file. Maybe we should use HH:MM timestamps
instead. Maybe we should rotate. Anyways, we should do something...

We might try to download the same nonexistent links on every run. What should
we do about that? It's possible the files will be there the next time (someone
might have uploaded them). Should we could them against the max num to download?
Should we refuse to retry failures for some number of runs (configurable)?

So, lots to do, even after a first kinda-working version is done.



